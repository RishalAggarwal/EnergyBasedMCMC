from typing import Callable, Iterable, NamedTuple, Tuple, Optional
from .prioritised_replay_buffer import SimpleReplayData,sample_without_replacement


import torch

class TimedReplayData(NamedTuple):
    """Log weights and samples generated by annealed importance sampling."""

    x: torch.Tensor
    energy: torch.Tensor
    time: torch.Tensor


class ReplayBufferTimed:
    
    def __init__(
        self,
        dim: int,
        max_length: int,
        min_sample_length: int,
        initial_sampler: Callable[[], Tuple[torch.Tensor, torch.Tensor]],
        device: str = "cpu",
        temperature: float = 1.0,
        fill_buffer_during_init: bool = True,
        sample_with_replacement: bool = False,
        num_integration_steps: int = 100,
        prioritize: bool = False,
    ):
              
        assert min_sample_length < max_length
        self.dim = dim
        self.max_length = max_length
        self.min_sample_length = min_sample_length
        self.num_integration_steps = num_integration_steps
        self.possible_indices = torch.arange(self.max_length).to(device)
        self.buffer = TimedReplayData(
            x=torch.zeros((self.max_length,self.num_integration_steps, self.dim)).to(device),
            energy=torch.zeros(
                (self.max_length,self.num_integration_steps)
            ).to(device),
            time=torch.zeros(
                (self.max_length,self.num_integration_steps,1)
            ).to(device),
        )
        self.final_timepoint_buffer = SimpleReplayData( x=torch.zeros((self.max_length, dim)).to(device), 
                                                       energy=torch.zeros(self.max_length).to(device))
        self.device = device
        self.current_index = 0
        self.current_add_count = 0
        self.is_full = False  # whether the buffer is full
        self.can_sample = False  # whether the buffer is full enough to begin sampling
        self.temperature = temperature 
        

        if fill_buffer_during_init:
            while self.can_sample is False:
                # fill buffer up minimum length
                x, log_w, t = initial_sampler()
                self.add(x, log_w, t)
        else:
            print("Buffer not initialised, expected that checkpoint will be loaded.")

        self.sample_with_replacement = sample_with_replacement
        self.prioritize = prioritize

    def __len__(self):
        return len(self.buffer)
    
    def __getitem__(self, idx):
        return self.buffer[idx]


    @torch.no_grad()
    def add(self, x: torch.Tensor, log_w: torch.Tensor, t: torch.Tensor) -> None:
        """Add a new batch of generated data to the replay buffer."""
        batch_size = x.shape[0]
        x = x.to(self.device)
        log_w = log_w.to(self.device)
        t = t.to(self.device)
        t.unsqueeze_(1)
        t=t.repeat_interleave(dim=1,repeats=x.shape[1])
        t.unsqueeze_(2)
        #x is [num_int_steps,batch_size, dim], t is [num_int_steps,batch_size,1], time to put batch_size first
        x = x.permute(1,0,2)
        t = t.permute(1,0,2)
        indices = (torch.arange(batch_size) + self.current_index).to(self.device) % self.max_length
        self.final_timepoint_buffer.x[indices] = x[:,-1,:]
        self.final_timepoint_buffer.energy[indices] = log_w[:,-1]
        self.buffer.x[indices] = x
        self.buffer.energy[indices] = log_w
        self.buffer.time[indices] = t
        new_index = self.current_index + batch_size
        if not self.is_full:
            self.is_full = new_index >= self.max_length
            self.can_sample = new_index >= self.min_sample_length
        self.current_index = new_index % self.max_length

    @torch.no_grad()
    def sample(
        self, batch_size: int, prioritize: Optional[bool] = None
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """Return a batch of sampled data, if the batch size is specified then the batch will have
        a leading axis of length batch_size, otherwise the default self.batch_size will be used."""
        if not self.can_sample:
            raise Exception("Buffer must be at minimum length before calling sample")

        if prioritize is None:
            prioritize = self.prioritize

        max_index = self.max_length if self.is_full else self.current_index
        if self.sample_with_replacement:
            if prioritize:
                indices = torch.distributions.Categorical(
                    logits=self.buffer.energy[:max_index]
                ).sample((batch_size,))
            else:
                indices = torch.randint(max_index, (batch_size,)).to(self.device)
        else:
            if prioritize:
                indices = sample_without_replacement(self.buffer.energy[:max_index], batch_size).to(
                    self.device
                )
            else:
                indices = torch.randperm(max_index)[:batch_size].to(self.device)
        #sample batch_size random time points
        random_time=torch.randint(self.num_integration_steps,(batch_size,)).to(self.device)
        x, log_w, t, indices = (
            self.buffer.x[indices,random_time],
            self.buffer.energy[indices,random_time],
            self.buffer.time[indices,random_time],
            indices,
        )
        return x, log_w, t, indices
    
    def sample_n_batches(
        self, batch_size: int, n_batches: int
    ) -> Iterable[Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]:
        """Returns a list of batches."""
        x, log_w, t, indices = self.sample(batch_size * n_batches)
        x_batches = torch.chunk(x, n_batches)
        log_w_batches = torch.chunk(log_w, n_batches)
        t_batches = torch.chunk(t, n_batches)
        indices_batches = torch.chunk(indices, n_batches)
        dataset = [
            (x, log_w, t, indxs)
            for x, log_w, t, indxs in zip(
                x_batches, log_w_batches, t_batches, indices_batches
            )
        ]
        return dataset
    
    def get_last_n_inserted(self, num_to_get: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        
        if self.is_full:
            assert num_to_get <= self.max_length
        else:
            assert num_to_get <= self.current_index

        start_idx = self.current_index - num_to_get
        idxs = [torch.arange(max(start_idx, 0), self.current_index)]
        if start_idx < 0:
            idxs.append(torch.arange(self.max_length + start_idx, self.max_length))

        idx = torch.cat(idxs)

        return self.final_timepoint_buffer.x[idx], self.final_timepoint_buffer.energy[idx]